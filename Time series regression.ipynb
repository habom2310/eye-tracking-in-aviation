{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import eye_metrics_utils\n",
    "import data_utils\n",
    "import gaze_entropy\n",
    "\n",
    "\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten, RepeatVector, TimeDistributed\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(\"data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_one = [v for v in csv_files if \"One Gaze-Vergence\" in v]\n",
    "csv_files_two = [v for v in csv_files if \"Two Gaze-Vergence\" in v]\n",
    "csv_files_three = [v for v in csv_files if \"Three Go-Around Gaze-Vergence\" in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "032 10 1A\n",
      "027 18 1A\n",
      "031 26 1A\n",
      "028 28 1A\n",
      "004 35 1A\n",
      "008 35 1A\n",
      "010 35 1A\n",
      "029 40 1A\n",
      "003 42 1A\n",
      "007 49 1A\n",
      "023 50 1A\n",
      "021 63 2B\n",
      "006 90 2B\n",
      "019 98 2B\n",
      "022 100 2B\n",
      "015 133 2B\n",
      "016 200 2B\n",
      "014 155 2C\n",
      "005 180 2C\n",
      "025 200 2C\n",
      "002 220 2C\n",
      "001 230 2C\n",
      "020 230 2C\n",
      "011 300 2C\n",
      "017 420 2C\n",
      "013 23 1A\n",
      "024 28 1A\n",
      "018 116 2B\n",
      "026 150 2C\n",
      "012 175 2C\n",
      "009 220 2C\n",
      "033 1300 2B\n"
     ]
    }
   ],
   "source": [
    "df_par = pd.read_csv(\"participant.csv\")\n",
    "par_id_arr = [v[-3:] for v in df_par['ID'].tolist()]\n",
    "flight_exp_arr = df_par['Flight_hour'].tolist()\n",
    "\n",
    "group_arr = df_par['Group'].tolist()\n",
    "\n",
    "for i,e,g in zip(par_id_arr, flight_exp_arr, group_arr):\n",
    "    print(i, e, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\PISSS_ID_001_Approach Two Gaze-Vergence.csv 9554\n",
      "data\\PISSS_ID_002_Approach Two Gaze-Vergence.csv 9430\n",
      "data\\PISSS_ID_003_Approach Two Gaze-Vergence.csv 9368\n",
      "data\\PISSS_ID_004_Approach Two Gaze-Vergence.csv 9862\n",
      "data\\PISSS_ID_005_Approach Two Gaze-Vergence.csv 9245\n",
      "data\\PISSS_ID_006_Approach Two Gaze-Vergence.csv 9739\n",
      "data\\PISSS_ID_007_Approach Two Gaze-Vergence.csv 9677\n",
      "data\\PISSS_ID_008_Approach Two Gaze-Vergence.csv 9923\n",
      "data\\PISSS_ID_009_Approach Two Gaze-Vergence.csv 9243\n",
      "data\\PISSS_ID_010_Approach Two Gaze-Vergence.csv 9923\n",
      "data\\PISSS_ID_011_Approach Two Gaze-Vergence.csv 9492\n",
      "data\\PISSS_ID_012_Approach Two Gaze-Vergence.csv 9431\n",
      "data\\PISSS_ID_013_Approach Two Gaze-Vergence.csv 8691\n",
      "data\\PISSS_ID_014_Approach Two Gaze-Vergence.csv 9307\n",
      "data\\PISSS_ID_015_Approach Two Gaze-Vergence.csv 8812\n",
      "data\\PISSS_ID_016_Approach Two Gaze-Vergence.csv 8259\n",
      "data\\PISSS_ID_017_Approach Two Gaze-Vergence.csv 9184\n",
      "data\\PISSS_ID_018_Approach Two Gaze-Vergence.csv 8937\n",
      "data\\PISSS_ID_019_Approach Two Gaze-Vergence.csv 9615\n",
      "data\\PISSS_ID_020_Approach Two Gaze-Vergence.csv 9923\n",
      "data\\PISSS_ID_021_Approach Two Gaze-Vergence.csv 9369\n",
      "data\\PISSS_ID_022_Approach Two Gaze-Vergence.csv 9307\n",
      "data\\PISSS_ID_023_Approach Two Gaze-Vergence.csv 9677\n",
      "data\\PISSS_ID_024_Approach Two Gaze-Vergence.csv 9307\n",
      "data\\PISSS_ID_025_Approach Two Gaze-Vergence.csv 8937\n",
      "data\\PISSS_ID_026_Approach Two Gaze-Vergence.csv 8259\n",
      "data\\PISSS_ID_027_Approach Two Gaze-Vergence.csv 8568\n",
      "data\\PISSS_ID_028_Approach Two Gaze-Vergence.csv 9554\n",
      "data\\PISSS_ID_029_Approach Two Gaze-Vergence.csv 10232\n",
      "data\\PISSS_ID_031_Approach Two Gaze-Vergence.csv 9923\n",
      "data\\PISSS_ID_032_Approach Two Gaze-Vergence.csv 8011\n",
      "data\\PISSS_ID_033_Approach Two Gaze-Vergence.csv 9245\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "def norm(df_x):\n",
    "    train_stats = df_x.describe().transpose()\n",
    "    return (df_x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "for csv_files in [csv_files_two]:\n",
    "#     part = defaultdict(dict)\n",
    "    for csv in csv_files:\n",
    "        par_id = csv[14:17]\n",
    "        \n",
    "        if par_id not in par_id_arr:\n",
    "            continue\n",
    "#         ret = defaultdict(list)\n",
    "        df_data = pd.read_csv(csv)\n",
    "        print(csv, len(df_data))\n",
    "        exp = flight_exp_arr[par_id_arr.index(par_id)]\n",
    "        group = group_arr[par_id_arr.index(par_id)]\n",
    "        for df_slice in data_utils.data_slicing(df_data, window_length = 1200, stride = 900, min_length=1200):\n",
    "            if (data_utils.check_percentage_null(df_slice) < 0.3): # Ignore data with missing value > 30%\n",
    "                continue\n",
    "                \n",
    "            df_slice.fillna(0.0, inplace=True)\n",
    "            v = df_slice[['X Pos', 'Y Pos', 'Pupil Diameter']]\n",
    "            v = norm(v)\n",
    "            \n",
    "            X.append(v.values)\n",
    "#             y.append(exp)\n",
    "            if \"1\" in group:\n",
    "                y.append(0)\n",
    "            elif \"2\" in group:\n",
    "                y.append(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimension: (293, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set dimension:\" ,np.array(X).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_loss_function(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "y_scale = scaler.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM_1 (LSTM)                (None, 900, 128)          67584     \n",
      "_________________________________________________________________\n",
      "LSTM_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "Fully_connected_1 (Dense)    (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "Fully_connected_2 (Dense)    (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 117,653\n",
      "Trainable params: 117,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(900, 3), return_sequences=True, name = \"LSTM_1\"))\n",
    "model.add(LSTM(64, name = \"LSTM_2\"))\n",
    "\n",
    "model.add(Dense(10, name = \"Fully_connected_1\"))\n",
    "model.add(Dense(1, name = \"Fully_connected_2\"))\n",
    "\n",
    "model.compile(loss=sqrt_loss_function, optimizer = tf.keras.optimizers.RMSprop(0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(900,3), name=\"Convolution_1D_1\"))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', name=\"Convolution_1D_2\"))\n",
    "model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling_1D_1\"))\n",
    "model.add(Flatten(name=\"Flatten_1\"))\n",
    "model.add(RepeatVector(1, name=\"Repeat_vector_1\"))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True, name = \"LSTM_1\"))\n",
    "model.add(Dense(10, activation='relu', name = \"Fully_connected_1\"))\n",
    "model.add(Dense(1, name = \"Fully_connected_2\"))\n",
    "\n",
    "model.compile(loss=sqrt_loss_function, optimizer = tf.keras.optimizers.RMSprop(0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-3,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 264 samples, validate on 89 samples\n",
      "Epoch 1/20\n",
      "264/264 [==============================] - 184s 698ms/step - loss: 0.6229 - val_loss: 0.7692\n",
      "Epoch 2/20\n",
      "264/264 [==============================] - 201s 761ms/step - loss: 0.6200 - val_loss: 0.7453\n",
      "Epoch 3/20\n",
      "264/264 [==============================] - 219s 828ms/step - loss: 0.5928 - val_loss: 0.8462\n",
      "Epoch 4/20\n",
      "264/264 [==============================] - 218s 826ms/step - loss: 0.5882 - val_loss: 0.7367\n",
      "Epoch 5/20\n",
      "264/264 [==============================] - 236s 894ms/step - loss: 0.5842 - val_loss: 0.7313\n",
      "Epoch 6/20\n",
      "264/264 [==============================] - 229s 869ms/step - loss: 0.5709 - val_loss: 0.7335\n",
      "Epoch 7/20\n",
      "264/264 [==============================] - 174s 657ms/step - loss: 0.5952 - val_loss: 0.7494\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=1, callbacks = callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7448194408684634"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = scaler.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.37021542937968"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(p.reshape(-1),t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61.130753] [150.]\n",
      "[50.474403] [63.]\n",
      "[101.49334] [155.]\n",
      "[82.87071] [35.]\n",
      "[68.68233] [155.]\n",
      "[78.103264] [100.]\n",
      "[95.23978] [100.]\n",
      "[85.31478] [116.]\n",
      "[73.85091] [155.]\n",
      "[203.2552] [10.]\n",
      "[107.48546] [150.]\n",
      "[95.22088] [150.]\n",
      "[88.10164] [40.]\n",
      "[53.844643] [300.]\n",
      "[86.05946] [175.]\n",
      "[76.96882] [220.]\n",
      "[94.4511] [100.]\n",
      "[74.71586] [18.]\n",
      "[59.89772] [42.]\n",
      "[79.085785] [200.]\n",
      "[80.74635] [98.]\n",
      "[53.358498] [50.]\n",
      "[86.18749] [63.]\n",
      "[84.85354] [220.]\n",
      "[78.805756] [90.]\n",
      "[81.36602] [63.]\n",
      "[74.46299] [18.]\n",
      "[77.06683] [100.]\n",
      "[80.3759] [42.]\n",
      "[58.388824] [155.]\n",
      "[145.64859] [116.]\n",
      "[52.66275] [18.]\n",
      "[69.40344] [420.]\n",
      "[84.73753] [220.]\n",
      "[49.519913] [220.]\n",
      "[86.84679] [28.]\n",
      "[50.270782] [116.]\n",
      "[105.251595] [100.]\n",
      "[97.012215] [26.]\n",
      "[96.22008] [98.]\n",
      "[89.778984] [26.]\n",
      "[79.86883] [35.]\n",
      "[86.602585] [40.]\n",
      "[77.36087] [35.]\n",
      "[68.30212] [200.]\n",
      "[78.28677] [26.]\n",
      "[89.83441] [28.]\n",
      "[46.9804] [49.]\n",
      "[83.907166] [180.]\n",
      "[97.42669] [40.]\n",
      "[72.61808] [28.]\n",
      "[70.14548] [175.]\n",
      "[90.23709] [220.]\n",
      "[189.37503] [420.]\n",
      "[98.94704] [116.]\n",
      "[85.926254] [10.]\n",
      "[60.769646] [49.]\n",
      "[67.0717] [35.]\n",
      "[39.668747] [230.]\n",
      "[65.00923] [35.]\n",
      "[50.243065] [420.]\n",
      "[86.48933] [23.]\n",
      "[81.56383] [180.]\n",
      "[41.802834] [50.]\n",
      "[80.29309] [35.]\n",
      "[82.61662] [300.]\n",
      "[67.22349] [18.]\n",
      "[60.672775] [300.]\n",
      "[78.58571] [63.]\n",
      "[115.41224] [150.]\n",
      "[83.15595] [35.]\n",
      "[88.14466] [230.]\n",
      "[100.38081] [220.]\n",
      "[40.89531] [40.]\n",
      "[79.078255] [10.]\n",
      "[84.9956] [200.]\n",
      "[52.94403] [180.]\n",
      "[60.81836] [35.]\n",
      "[48.047127] [300.]\n",
      "[51.112434] [300.]\n",
      "[61.449516] [35.]\n",
      "[66.68787] [49.]\n",
      "[48.24472] [230.]\n",
      "[124.04889] [220.]\n",
      "[71.033646] [100.]\n",
      "[218.9298] [50.]\n",
      "[68.711006] [200.]\n",
      "[82.73618] [49.]\n",
      "[112.54514] [150.]\n"
     ]
    }
   ],
   "source": [
    "for u,v in zip(p,t):\n",
    "    print(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM_1 (LSTM)                (None, 1200, 128)         67584     \n",
      "_________________________________________________________________\n",
      "LSTM_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "Fully_connected_2 (Dense)    (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 117,057\n",
      "Trainable params: 117,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(1200, 3), return_sequences=True, name = \"LSTM_1\"))\n",
    "model.add(LSTM(64, name = \"LSTM_2\"))\n",
    "\n",
    "# model.add(Dense(10, name = \"Fully_connected_1\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\", name = \"Fully_connected_2\"))\n",
    "\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Convolution_1D_1 (Conv1D)    (None, 1198, 128)         1280      \n",
      "_________________________________________________________________\n",
      "Convolution_1D_2 (Conv1D)    (None, 1196, 64)          24640     \n",
      "_________________________________________________________________\n",
      "MaxPooling_1D_1 (MaxPooling1 (None, 598, 64)           0         \n",
      "_________________________________________________________________\n",
      "Flatten_1 (Flatten)          (None, 38272)             0         \n",
      "_________________________________________________________________\n",
      "Repeat_vector_1 (RepeatVecto (None, 1, 38272)          0         \n",
      "_________________________________________________________________\n",
      "LSTM_1 (LSTM)                (None, 128)               19661312  \n",
      "_________________________________________________________________\n",
      "Fully_connected_1 (Dense)    (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "Fully_connected_2 (Dense)    (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 19,688,533\n",
      "Trainable params: 19,688,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(1200,3), name=\"Convolution_1D_1\"))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', name=\"Convolution_1D_2\"))\n",
    "model.add(MaxPooling1D(pool_size=2, name=\"MaxPooling_1D_1\"))\n",
    "model.add(Flatten(name=\"Flatten_1\"))\n",
    "model.add(RepeatVector(1, name=\"Repeat_vector_1\"))\n",
    "model.add(LSTM(128, activation='relu', name = \"LSTM_1\"))\n",
    "model.add(Dense(10, activation='relu', name = \"Fully_connected_1\"))\n",
    "model.add(Dense(1, activation='sigmoid', name = \"Fully_connected_2\"))\n",
    "\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 175 samples, validate on 59 samples\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 37s 213ms/step - loss: 0.6777 - accuracy: 0.5886 - val_loss: 0.7162 - val_accuracy: 0.5085\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 37s 212ms/step - loss: 0.6506 - accuracy: 0.6229 - val_loss: 0.7211 - val_accuracy: 0.4915\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 38s 219ms/step - loss: 0.6470 - accuracy: 0.6400 - val_loss: 0.7278 - val_accuracy: 0.5085\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=8, callbacks = callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.6\n",
    "\n",
    "pred = []\n",
    "for v in prob:\n",
    "    if v > thresh:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 [0.621034]\n",
      "1 1 [0.7384903]\n",
      "0 1 [0.58146125]\n",
      "0 1 [0.5038871]\n",
      "1 0 [0.7363177]\n",
      "0 0 [0.58066565]\n",
      "0 0 [0.5799566]\n",
      "0 1 [0.58131284]\n",
      "1 0 [0.87280875]\n",
      "1 0 [0.66131806]\n",
      "1 0 [0.6499718]\n",
      "1 0 [0.66942483]\n",
      "0 1 [0.58417046]\n",
      "1 1 [0.809629]\n",
      "0 1 [0.52126116]\n",
      "1 0 [0.6027039]\n",
      "1 1 [0.73406214]\n",
      "1 1 [0.6811725]\n",
      "1 1 [0.60747516]\n",
      "1 1 [0.78821087]\n",
      "1 1 [0.6938323]\n",
      "0 0 [0.54722005]\n",
      "1 0 [0.6938436]\n",
      "1 1 [0.607084]\n",
      "1 0 [0.737562]\n",
      "1 0 [0.8020246]\n",
      "1 1 [0.6811895]\n",
      "1 1 [0.6436151]\n",
      "1 1 [0.6456598]\n",
      "1 0 [0.62175244]\n",
      "1 0 [0.67832315]\n",
      "1 1 [0.62808204]\n",
      "0 0 [0.53077376]\n",
      "0 1 [0.5853143]\n",
      "1 0 [0.62011635]\n",
      "0 1 [0.5894805]\n",
      "1 1 [0.69877696]\n",
      "1 0 [0.703298]\n",
      "0 0 [0.5426033]\n",
      "1 0 [0.66014796]\n",
      "1 1 [0.70668316]\n",
      "1 1 [0.84573853]\n",
      "0 0 [0.56580323]\n",
      "1 0 [0.6592716]\n",
      "1 0 [0.8308723]\n",
      "1 0 [0.6108274]\n",
      "1 1 [0.67751896]\n",
      "1 1 [0.7404653]\n",
      "0 0 [0.50671816]\n",
      "0 1 [0.5907571]\n",
      "1 1 [0.6215226]\n",
      "1 0 [0.63665813]\n",
      "1 0 [0.6228131]\n",
      "0 0 [0.59186006]\n",
      "0 0 [0.57529914]\n",
      "0 0 [0.56175154]\n",
      "0 1 [0.51340914]\n",
      "1 0 [0.72043115]\n",
      "1 0 [0.6433616]\n"
     ]
    }
   ],
   "source": [
    "for u,v,p in zip(pred,y_test,prob):\n",
    "    print(u,v,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     group 1       0.53      0.32      0.40        31\n",
      "     group 2       0.47      0.68      0.56        28\n",
      "\n",
      "    accuracy                           0.49        59\n",
      "   macro avg       0.50      0.50      0.48        59\n",
      "weighted avg       0.50      0.49      0.48        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['group 1', 'group 2']\n",
    "print(classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
