{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygaze\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import detectors\n",
    "import gazeplotter\n",
    "from collections import defaultdict\n",
    "# import local lib\n",
    "import eye_metrics_utils\n",
    "import data_utils\n",
    "import gaze_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(df_data):\n",
    "    df_x = df_data.copy()\n",
    "    if (data_utils.check_percentage_null(df_x) < 0.5): # if missing value > 50%, remove\n",
    "        return None\n",
    "    \n",
    "    time = np.array(df_data['Start Time (secs)'].tolist())\n",
    "\n",
    "    Efix = eye_metrics_utils.detect_fixations(df_x)\n",
    "    Eblk = eye_metrics_utils.detect_blinks(df_x)\n",
    "    Esac = eye_metrics_utils.detect_saccades(df_x)\n",
    "    Emsac = eye_metrics_utils.detect_microsaccades(df_x)\n",
    "#     print(Efix)\n",
    "    X = np.array(Efix).T[3:].T\n",
    "    Hs, Ht = gaze_entropy.entropy(X)\n",
    "    total_time = time[-1] - time[0]\n",
    "    \n",
    "    return Efix, Hs, Ht, total_time, Eblk, Esac, Emsac\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob(\"data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_one = [v for v in csv_files if \"One Gaze-Vergence\" in v]\n",
    "csv_files_two = [v for v in csv_files if \"Two Gaze-Vergence\" in v]\n",
    "csv_files_three = [v for v in csv_files if \"Three Go-Around Gaze-Vergence\" in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par = pd.read_csv(\"participant.csv\")\n",
    "group = [df_par[df_par['Group'].str.contains(\"1\")]['ID'].tolist(), df_par[df_par['Group'].str.contains(\"2\")][\"ID\"].tolist()]\n",
    "group = [[i[-3:] for i in v] for v in group]\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_groups = []\n",
    "for g in group:\n",
    "    trials = []\n",
    "    for csv_files in [csv_files_one, csv_files_two, csv_files_three]:\n",
    "        ret = defaultdict(list)\n",
    "        for csv in csv_files:\n",
    "            par_id = csv[14:17]\n",
    "            if par_id not in g:\n",
    "                continue\n",
    "                \n",
    "            df_data = pd.read_csv(csv)\n",
    "            print(csv, len(df_data))\n",
    "\n",
    "            for v in data_utils.data_slicing(df_data, stride = 1200):\n",
    "                r = run_all(v)\n",
    "                if r != None:\n",
    "                    Efix, Hs, Ht, total_time, Eblk, Esac, Emsac = r\n",
    "                    ret[\"Eblk\"].append(Eblk)\n",
    "                    ret[\"Efix\"].append(Efix)\n",
    "                    ret[\"Esac\"].append(Esac)\n",
    "                    ret[\"Emsac\"].append(Emsac)\n",
    "#                     ret[\"trans_matrix\"].append(trans_matrix)\n",
    "                    ret[\"Hs\"].append(Hs)\n",
    "                    ret[\"Ht\"].append(Ht)\n",
    "                    ret[\"total_time\"].append(total_time)\n",
    "        trials.append(ret)\n",
    "    feature_groups.append(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.DataFrame()\n",
    "for j, g in enumerate(feature_groups):\n",
    "    fix_dur = []\n",
    "    for i, p in enumerate(g[1]['Efix']):\n",
    "        fix_dur = np.append(fix_dur,np.mean(np.array(p).T[2]))\n",
    "        \n",
    "    Hs = g[1]['Hs']\n",
    "    Ht = g[1]['Ht']\n",
    "    \n",
    "    fix_rate = np.array([len(v) for v in g[1]['Efix']])/np.array(g[1]['total_time'])\n",
    "    blk_rate = np.array([len(v) for v in g[1]['Eblk']])/np.array(g[1]['total_time'])\n",
    "    sac_rate = np.array([len(v) for v in g[1]['Esac']])/np.array(g[1]['total_time'])\n",
    "    msac_rate = np.array([len(v) for v in g[1]['Emsac']])/np.array(g[1]['total_time'])\n",
    "\n",
    "    print(len(fix_dur), len(fix_rate), len(sac_rate), len(blk_rate), len(msac_rate))\n",
    "    group = j*np.ones_like(Hs)\n",
    "    df = pd.DataFrame(zip(fix_dur, Hs, Ht, fix_rate, blk_rate, sac_rate, msac_rate, group), \n",
    "                        columns=[\"fix_dur\", \"Hs\", \"Ht\", \"fix_rate\", \"blk_rate\", \"sac_rate\", \"msac_rate\", \"group\"]).astype({\"group\":\"int\"})\n",
    "    df_x = pd.concat([df_x, df])\n",
    "    \n",
    "df_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_x[~(df_x.iloc[:,:7] == 0).any(axis=1)]\n",
    "len(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_x[['fix_dur', 'Hs', 'Ht', \"fix_rate\", \"blk_rate\", \"sac_rate\", \"msac_rate\"]].values\n",
    "# X = df_x[['fix_dur', \"fix_rate\"]].values\n",
    "\n",
    "y = df_x[['group']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\n",
    "\n",
    "clf = LogisticRegression(random_state=1).fit(X_train, y_train)\n",
    "# clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['fix_dur', 'Hs', 'Ht', \"fix_rate\", \"blk_rate\", \"sac_rate\", \"msac_rate\"]\n",
    "print(clf.classes_)\n",
    "for i in range(len(features)):\n",
    "    print(features[i] + \"      \\t: \" + str(round(clf.coef_[0][i],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "target_names = ['group 1', 'group 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC: \", roc_auc_score(y_test, y_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(2),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "\n",
    "# preprocess dataset, split into training and test part\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# just plot the dataset first\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "ax = plt.subplot(1, len(classifiers) + 1, i)\n",
    "ax.set_title(\"Input data\")\n",
    "# Plot the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "# Plot the testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "           edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "i += 1\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    ax = plt.subplot(1, len(classifiers) + 1, i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "\n",
    "    print(name)\n",
    "    print(\"AUC: \", roc_auc_score(y_test, y_score))\n",
    "    target_names = ['group 1', 'group 2']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "#     if hasattr(clf, \"decision_function\"):\n",
    "#         Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     else:\n",
    "#         Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "#     # Put the result into a color plot\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "#     # Plot the training points\n",
    "#     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "#                edgecolors='k')\n",
    "#     # Plot the testing points\n",
    "#     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "#                edgecolors='k', alpha=0.6)\n",
    "\n",
    "#     ax.set_xlim(xx.min(), xx.max())\n",
    "#     ax.set_ylim(yy.min(), yy.max())\n",
    "#     ax.set_xticks(())\n",
    "#     ax.set_yticks(())\n",
    "#     ax.set_title(name)\n",
    "#     ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "#             size=15, horizontalalignment='right')\n",
    "#     i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,3,4,5,6,2,1,3,5,2,3,1],[1,2,3,1,2,3,4,5,3,2,2,3]]\n",
    "\n",
    "a = StandardScaler().fit_transform(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
